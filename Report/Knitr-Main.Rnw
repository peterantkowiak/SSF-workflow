\documentclass[11pt, a4paper]{article} %or article has only section and below, book and report also have chapter: http://texblog.org/2007/07/09/documentclassbook-report-article-or-letter/

\usepackage[utf8]{inputenc}  % use utf8 encoding of symbols such as umlaute for maximal compatibility across platforms

\usepackage{caption}  			% provides commands for handling caption sizes etc.
%\usepackage[a4paper, left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}		 % to easily change margin widths: https://www.sharelatex.com/learn/Page_size_and_margins

\usepackage{etoolbox}    % for conditional evaluations!
\usepackage[bottom]{footmisc}  % I love footnotes! And they should be down at the bottom of the page!
\usepackage{graphicx}        % when using figures and alike
\usepackage[hidelinks]{hyperref}		% for hyperreferences (links within the document: references, figures, tables, citations)

\usepackage{euler}     % a math font, only for equations and alike; call BEFORE changing the main font; alternatives: mathptmx, fourier, 
%\usepackage{gentium} % for a different font; you can also try: cantarell, charter, libertine, gentium, bera, ... http://tex.stackexchange.com/questions/59403/what-font-packages-are-installed-in-tex-live

\usepackage{pifont}

%\usepackage{listings}
%\lstset{breaklines=T}

%------------------------------------------------------------------------------------------------------
%------- text size settings --------------
\setlength{\textwidth}{16cm}% 
\setlength{\textheight}{25cm} %23 
%(these values were used to fill the page more fully and thus reduce the number of pages!)
\setlength{\topmargin}{-1.5cm} %-1.5
\setlength{\footskip}{1cm} %
%\setlength{\hoffset}{0cm} %
\setlength{\oddsidemargin}{0cm}%1
\setlength{\evensidemargin}{0cm} %-0.5
\setlength{\parskip}{0cm} % Abstand zwischen Absätzen
% ----------------------------------------------------------------
\renewcommand{\textfraction}{0.1} % allows more space to graphics in float
\renewcommand{\topfraction}{0.85}
%\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.70}


\frenchspacing %http://texwelt.de/wissen/fragen/1154/was-ist-french-spacing-was-macht-frenchspacing

\sloppy

%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------

\begin{document}

%%%%%%%%%%%%% this bit is new to Knitr: %%%%%%%%%%%%%%%%%%%%%
<<setup, include=FALSE, cache=FALSE, echo=F>>=
library(knitr)
# library(formatR)
# set global chunk options
opts_chunk$set(fig.path='', fig.align='center', fig.show='hold', tidy=FALSE)
options(replace.assign=TRUE, width=80)
#render_listings()
@


\title{A tutorial for implementing Step Selection Function in R}

\author{P. Antkowiak\thanks{M.Sc. programme ``GIS und Umweltmodellierung'' at University of Freiburg} \and H. Tripke\thanks{M.Sc. programme ``Wildlife, Biodiversity and Vegetation'' at University of Freiburg} \and C. Wilhelm\thanks{M.Sc. programme ``Wildlife, Biodiversity and Vegetation'' at University of Freiburg}}
% for more control, multiple affiliations, line breaks and alike, use the authblk package!!

\date{\today} % !!use package isodate for more control of date formatting!!

\maketitle

%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------

\tableofcontents

\newpage

\section{Introduction}%------------------------------------------------------------------------------------------------------

\subsection{Purpose and applications of SSFs}

\noindent In addition to Resources Selection Functions (RSF) another powerful tool for evaluating data on animal movements and habitat selection are Step Selection Functions (SSF). The latter are used to estimate resource selection by comparing observed habitat use with available structures. Given GPS locations of a collared individual are connected by a linear segment. These segments are considered as steps. The time intervals influencing the step length should be choosen carefully (i.e. by conducting a pilot study) to meet the requirements of the study questions, the target species and its behaviour. The SSF then calculates random steps by taking measured angle and distance along steps and using the observed positions as starting points. These alternative steps represent the available habitat, which could have been chosen, within a realistic step length of the observed positions. Finally, we can compare spatial attributes on  both and test for effects that explain habitat selection by animals \cite{thurfjell2014applications}.\\ So far, SSF models were mainly done using Geospatial Modelling Environment (GME) that works with a GIS\footnote{www.spatialecology.com/gme/}. However, more and more packages for analyzing animal movements are provided in R. None of these packages is designed for doing a SSF only but quite a number provide already helpful functions to perform single steps of the Selection Function. Therefore, the aim of this tutorial is to collect all functions necessary to conduct a SSF and order them in a  way that intuitively makes you understand how to run a SSF with your own data. Each step will be explained using an exemplary dataset of GPS locations collected from seven Cougars (\textit{Puma concolor}) in the year 2010 (in the following adressed as ``xmpl'').\\

\subsection{Our SSF workflow in R}

\noindent Figure~\ref{fig:Flowchart} provides an overview of all necessary steps and potential options to conduct a SSF. This tutorial will guide you through each step and gives brief instructions on how to implement the functions and what to consider beforehand.
To conduct a SSF using this tutorial we need you to store your initial data  in two independant datasets: \begin{enumerate} \item {A raster file of your spatial attributes (\emph{Raster data})} and \item{GPS locations of your individuals assigned with a time stamp (\emph{Waypoint data})}. \end{enumerate} 
We will start with the \emph{Waypoint data} because these need to be transformed a couple of times to be able to work with them. You can find the single steps on the right site of Figure~\ref{fig:Flowchart}. While there are many options to adjust your \emph{Waypoint data} the \emph{Raster data} describing your spatial attributes needs not much of change. Once you created random steps for your observed positions you can extract the spatial attributes for each of those positions by using the function \texttt{extract}. At this point, \emph{Waypoint} and \emph{Raster data} will be combined and your final model can be written. \newpage


\begin{figure} 
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=1\textwidth]{Flowchart.pdf} % our perfect workflow!
\caption{Stepwise conducting a Step Selection Function using existing R-packages. The yellow boxes show the name of the function applied while the blue boxes provide the type of object or data. In light grey optional steps are highlighted that are not implemented in this tutorial.}
\label{fig:Flowchart}
\end{figure}


\subsection{Installing and loading Packages}

\noindent Before you can actually start using this tutorial for conducting SSF you need to install a bunch of packages in R. Some of them require others so that you have to add all these to your library:

<< eval=FALSE, warning=FALSE, message=FALSE>>=

## for implementing SSF
install.packages("hab")
install.packages("hab", repos = "http://ase-research.org/R/") # regular
install.packages("hab", repos = "http://ase-research.org/R/",
                 type = "source")                             # for self-compiling
@

\noindent Keep fingers off the \texttt{adehabitat} package! It is outdated and replaced by four different packages designed for different analyses:

<< eval=FALSE, warning=FALSE, message=FALSE>>=
install.packages("adehabitatHR") # dealing with home ranges
install.packages("adehabitatHS") # habitat selection
install.packages("adehabitatLT") # trajectories
install.packages("adehabitatMA") # maps

install.packages("tkrplot")

# for handling raster data
install.packages("move")
install.packages("raster")
install.packages("rgdal")

# for analyzing the data
install.packages("mclogit")
install.packages("lme4")
install.packages("effects")
@
\noindent For faster processing the \texttt{install.packages} function is deactivated.\\

\noindent Loading packages:

<< eval= TRUE, warning=FALSE, message=FALSE>>=
library(hab)
library(adehabitatMA)
library(adehabitatHR)
library(adehabitatHS)
library(adehabitatLT)

library(sp)
library(raster)

library(mclogit)
library(lme4)
library(effects)
@



\section{Processing the Waypoint Data}

\subsection{Loading Waypoint Data (*.csv, ESRI)}%------------------------------------------------------------------------------------------------------

\noindent The data for the analysis should be saved in a simple *.csv file format. The table should have headings for each column in the first row and each observation should include at least the four following values: 
\begin{itemize}
\item{animal ID}
\item{x-coordinate (easting)}
\item{y-coordinate (northing)}
\item{date and/or time}
\end{itemize}

\noindent Remember that the coordinates need to be provided in the same coordinate system and spatial projection as the raster data.\\

\noindent Depending on your analysis you can include further values such as:
\begin{itemize}
\item{ID for each record}
\item{GPS precision}
\item{other recording parameters such as season or month}
\item{temperature / elevation at the moment of record}
\item{other values that might be of interest in the further analysis}
\end{itemize}


\noindent Use the following commands to set your working directory and read the data:
<<>>=
setwd("P:/Henriette/BestPracticeR/SSF-workflow/Code")
xmpl = read.csv("UTMsREDUCED.csv", head=T)
@

\noindent You can execute \texttt{head(xmpl)} and \texttt{str(xmpl)} to check, whether the data was successfully read.


\subsection{Creating a ``Spatial Points Data Frame''}%------------------------------------------------------------------------------------------------------
\noindent The functions used along the rest of the toolchain can only process data which is stored as an object of class ``SpatialPointsDataFrame''. This object class stores the coordinates separately and can be created using the according function from package \texttt{sp}. 

<< warning=FALSE, message=FALSE>>=
library(sp)

xmpl.spdf = SpatialPointsDataFrame(coords = xmpl[,c("easting","northing")],
                                   data = xmpl)
@
\noindent In the \texttt{coords} argument of the function you should specify the ``x'' and ``y'' values for your dataset. In our example, we simply assign the two coordinate columns of the example dataset, but you can read the coordinates from a separate file if you want.\\

\noindent To see, what information is stored in the data frame, use the \texttt{names} command:

<< warning=FALSE, message=FALSE>>=
names(xmpl)
@


\subsection{Creating an ``ltraj'' object}%------------------------------------------------------------------------------------------------------
\noindent After storing the data in a ``Spatial Points Data Frame'', you now need to connect the single points and turn them into a set of trajectories. This operation is carried out by the function \texttt{as.ltraj} from the``hab'' package and produces objects of class ``ltraj''. There are two different types of trajectories. For \textbf{Type I} the time is not known or not taken into account. \textbf{Type II} is characterized by a timestamp. If the time lag between the different locations is the same, it is called ``regular'', if not ``irregular''.
The function \texttt{as.ltraj} requires at least three arguments to work:
\begin{enumerate}
\item{``x'' and ``y'' (x- and y- coordinates for each point)}
\item{``date'' (timestamp for each point, given as ``POSIXct'' class)}
\item{``id'' (the animal id)}
\end{enumerate}

\noindent Both, coordinates and animal id can easily be adopted from the ``Spatial Points Data Frame''. The timestamp however, needs to be stored as a ``POSIXct'' value with date and time in the same cell. If it is not stored in the required format yet, you therefore need to convert it first:

<<>>=
date <- as.POSIXct(strptime(paste(xmpl.spdf$LMT_DATE, xmpl.spdf$LMT_TIME),
                            "%d/%m/%Y  %H:%M:%S"))
@
\noindent If your dataset already features a ``POSIXct'' timestamp, you can skip this step.
Now you can proceed and actually create the ``ltraj'' object by executing the following command:
<<>>=
xmpl.ltr <- hab:::as.ltraj(xy = xmpl.spdf@coords, date = date,
                           id = xmpl.spdf$cat) 
@
\noindent Two comments to the function used: By typing \texttt{hab:::as.ltraj} you tell R to use the \texttt{as.ltraj} function from the ``hab'' package which is speed optimized against its \texttt{adehabitatLT} sibling. Unlike the \texttt{xmpl.spdf@coords} promt which works for any ``SPDF'' object, the \texttt{xmpl.spdf\$cat} prompt is specific to your dataset. In the example dataset, animal ID's are stored as an integer vector called \texttt{cat}. If this differs in your dataset and you should change the prompt accordingly. The \texttt{id} creates subsets in your data set which you can see in the structure below.

\noindent You now may want to have a closer look at the created ``ltraj'' object. Display its structure by executing \texttt{str(xmpl.ltr)}. The ``ltraj'' object is a list containing seven different dataframes, each for one individual, which we defined with the \texttt{id}. In each dataframe all information is stored for every single observation of the individual: 
\begin{itemize}
\item{\texttt{x} and ``y'' (x- and y- coordinates for each point)}
\item{\texttt{date} (timestamp for each point, given as POSIXct class)}
\item{\texttt{dx} and ``dy'' (changes in x- and y- values)}
\item{\texttt{dist} (length of trajectory)}
\item{\texttt{dt} (time between relocations in s)}
\item{\texttt{R2n} (squared net displacement between current relocation and first relocation)}
\item{\texttt{abs.angle} (absolute angle of trajectory)}
\item{\texttt{rel.angle} (angle between previous and actual trajectory)}
\end{itemize}


<<>>=
str(xmpl.ltr)
@

\noindent To work with one of the tables only you can use []. This is still an ``ltraj'' object. For example:
<<>>=
xmpl.ltr[2]
@

\noindent If you want to have a look at all the data stored for one single individual, or ``use'' it as a data frame you can use [[]]. If you assign it to a name, it is automatically converted and ready to use:
<<>>=
xmpl.df.10286 = xmpl.ltr[[1]]
@


\noindent To get a visual impression of your data you can plot the trajectory for all or for one particular animal:

<<ltraj_plot, echo=TRUE, dev="pdf", fig.show="hide">>=
plot(xmpl.ltr)
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:ltraj_plot}.

\begin{figure}[!htbp]
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{ltraj_plot.pdf}
\caption{Visualization of the observed positions and the path from seven collared cougars.}
\label{fig:ltraj_plot}
\end{figure}

\noindent To get a list of all cat IDs, use the \texttt{unique} command. Choose one that you are interested in!
<<>>=
unique(xmpl.spdf$cat) 
    
@

\noindent You can then look at the trajectory of a single individual only.
<<ind_plot, echo=TRUE, dev="pdf", fig.show="hide">>=
plot(xmpl.ltr, id=10289)
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:ind_plot}.

\begin{figure}[!htbp]
\captionsetup{width=0.6\textwidth}
\centering
\includegraphics[width=0.6\textwidth]{ind_plot.pdf}
\caption{Visualization of the observed positions and the path from only one individual.}
\label{fig:ind_plot}
\end{figure}


\subsection{Creating Bursts}%

\noindent As described in the previous section, the relocations stored in the ``ltraj'' object are already devided into the different individuals. This partition is called a ``burst''. For analysing the data, there might be the need to create ``sub-bursts'' for each animal within your trajectory. For example, if the animals were only recorded during the day, the monitoring took place over two consecutive years or the time lag between the relocations differs remarkably, each accumulation of relocations can be defined as a different burst. Looking at those different parts seperately might be necessary for different reasons. It is also possible to split your data into homogenous behaviour. Suggestions how to do that are explained in one of the following sections.\\
\noindent The function \texttt{cutltraj} splits the given bursts of your ``ltraj'' object into smaller bursts according to a specified criterion. In contrast, the function \texttt{bindltraj} combines the bursts of an object of class ``ltraj'' with the same attribute \texttt{id} to one unique burst. 


\noindent To find out if there are missing values and to get an overview of the time lags between the relocations, you can plot the changes of your trajectory over time \texttt{dt}. For that, you need to define the time interval you are looking at \cite{0.3.162014}.

\noindent In our example, the locations of the cougars were recorded every 3 hours, starting at 3 AM. The location at midnight is always missing. As \texttt{dt}, the time between successive relocations, is measured in seconds, we need to convert it into 3 hours.

<<time_seq, echo=TRUE, dev="pdf", fig.show="hide">>=
plotltr(xmpl.ltr, "dt/3600/3")
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:time_seq}. The relocations which are 3 hours apart, are plottet at value 1 on the y-axis, if one relocation is missing (time lag of 6 hours) the relocation gets the value 2. As you can see, there are a lot of relocations missing. To keep the time lag constant, we now want to split the existing bursts (individuals) into ``sub-bursts'' where the time lag is bigger than 3 hours. A constant time lag is necessary to have comparable trajectories for further analyses.

\begin{figure}[!htbp]
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{time_seq.pdf}
\caption{For each individual the observations are plotted. The y-axis shows the time interval from one observation to the next. Observations at 1 means that they are not more than 3 hours apart.}
\label{fig:time_seq}
\end{figure}

\noindent To cut our data at our desired interval, we need a function which defines \texttt{dt}. Because we want to keep relocations which are only a few minutes ``wrong'', we added 10 extra minutes.\\
\noindent Note: We still keep data with different time lags (< 3:10 h). As this is an irregular type II trajectory, it might cause problems with different functions (e.g. checking for autocorrelation). There are functions to convert it into a regular one, look at the \texttt{adehabitatLT} tutorial by C. Calenge \cite{0.3.162014}, the help section to get more information or define an explicit time lag in the function below. Because this however might lead to a massive loss of data, we didn't do that. 

<<>>=
foo = function(dt) { return(dt> (3800*3))} 
@

\noindent Then we split the object of class ltraj into smaller bursts using \texttt{cutltraj} and the function above. The bursts we had before applying this function still remain.

<<>>=
xmpl.cut <- cutltraj(xmpl.ltr, "foo(dt)", nextr = TRUE)
@
\noindent Note the Warning Message! As many recordings are missing we are losing a lot of data.

\noindent There are two options of cutting the trajectory depending on nextr. If it is set as FALSE, the burst stops before the first relocation matching the criterion. if it is set as TRUE, it stops after. \cite{0.3.162014}


\subsection{Distinguish different behaviors: Broken stick model and autocorrelation}
NEEDS EXPLANATION!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Autocorrelation between dist or angle means homogenous behaviour

\subsection{Creating Random Steps}

\noindent Given your final bursts we now randomly draw angles and distances from your observed data to get random steps. The angle is taken from the previous trajectory to the actual one while the distance is taken from the starting point to the next observed position. This means that each burst should at least contain three observed positions to create random steps for one relocation. The first and the last relocations only contribute to the calculation but don't have their ``own'' random steps as there is no previous trajectory an angle could be calculated from. Neither there is an endpoint to draw a distance. This means that the number of poins with random steps is lower as your actual recorded relocations.
Before applying the function \texttt{rdSteps} you might want to check for correletaion between turning angle and distance. In case your individuals tend to move long distances by turning only in small angles (e.g. a species migrating) but stop for several days for feeding you want to pick the distance and the angle as pairs. If no correlation is found you can pick both variables independently.
To check for this we use the \texttt{plot} function but first have to convert our ``ltraj'' object back to a data frame by using \texttt{ld}.

<<>>=
# ld is a quick way to create a data frame from an ltraj object
xmpl.cut.df <- ld(xmpl.cut)  
   
@


<<cor_1, echo=TRUE, dev="pdf", fig.show="hide">>=
plot(xmpl.cut.df$dist, xmpl.cut.df$rel.angle)
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:cor_1}.

\begin{figure}[!htbp]
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.6\textwidth]{cor_1.pdf}
\caption{Testing for correlation of the observed turning angle and step length. The y-axis shows the difference in the turning angle. The shape indicates a correlation for longer steps and doing only small turning angles (around 1).}
\label{fig:cor_1}
\end{figure}


\noindent The plot shows a correlation of step length and turning angle and therefore the random steps should be taken as pairs (\texttt{simult = T}). Per default the angle and distance for each random step is drawn from the observed values you provide with \texttt{x}. If your random steps shall be taken from a different dataset you can do so by writting it in \texttt{rand.dist = YourDataSet}. Hereby, you can also specify a distribution for estimating angle and distance. In our case we stick to the same dataset and apply \texttt{rdSteps}.  

<<>>=
  
xmpl.steps <- rdSteps(x = xmpl.cut, nrs = 10, simult = T, rand.dis = NULL,  
                      distMax = Inf, reproducible = TRUE, only.others = FALSE) 
          # use simult = FALSE if your data is not correlated
@

\noindent The function \texttt{rdSteps} uses some default settings which offer you options to modify your random steps. You can for example, easily change the number of steps taken from the observed data by defining \texttt{nrs} (default is 10) or if you only need steps shorter than a certain value specify \texttt{distMax} to that value (per default all steps are taken). By setting \textt{reproducible = TRUE} a seed is used to get reproducible random steps. If you want to exclude your current individual to draw angle and distance from this set include \texttt{only.others = TRUE}.
\noindent All in all, \texttt{rdSteps} is very straight forward and computes a lot of useful things for you: 

<<>>=
head(xmpl.steps)
@

\noindent The table still includes your ``cat id'', ``burst id'', the ``rel.angle'' and ``dist'' of your observed positions. Furthermore, the ``case'' is provided as categories of 0 and 1 for available and used. The ``strata'' marks all 10 random steps and the visited location so you can later tell your function what to compare.  
Depending on your analysis you might want to compare only your observed positions with the endpoints of your random steps or you want to investigate in the selection of spatial attributes along the path. For latter you need to implement more code because we did not use this option in our tutorial (see the grey box in Figure~\ref{fig:Flowchart}).\\
Now only new coordinates for your random points are missing. Instead two columns provide the differences of your x- and y- coordinates for each random step (``dx'' and ``dy''). To get new coordinates for your random steps we simply add these differences to your initial coordinates and create two new columns. 

<<>>=
xmpl.steps$new_x <- xmpl.steps$x + xmpl.steps$dx
xmpl.steps$new_y <- xmpl.steps$y + xmpl.steps$dy
@
  
\noindent After running this chapter you get your final ``SpatialPointDataFrame'' to use with the selection function.

<<>>=
head(xmpl.steps)
@


\subsection{Different distributions as an option for rdSteps}

!!!!!!!!!!!!!!!NEEDS FURTHER EXPLANATION!!!!!!!!!!!!!!!!!!!!!!!!!!
\\


\section{Processing Spatial Covariates}%----------------------------------------------------------------------------------------------------------------------------
\noindent This section explains the handling of spatial parameters that will be tested for selection by the target species. You should store these data in raster files (ESRI *.adf or georeferenced *.tif). These should have the same coordinate system as your telemetry data and should (for time reasons) already be clipped to your study area. For instructions how to do this in R, please read the GIS instructions from the other group ;)   

\subsection{Load Raster Data (ESRI, *.tif, (*.shp))}%------------------------------------------------------------------------------------------------------

\noindent With a simple function stored in the package \texttt{raster} you are able to upload any raster file into R. Examplarily we use raster data on the following parameters for the study area:
\begin{itemize}
\item{ruggedness of the terrain}
\item{land cover}
\item{canopy cover}
\item{distance to the nearest highway}
\item{distance to the nearest road}
\end{itemize}

\noindent For reading the raster data, three packages are required:
<<echo=FALSE, message=FALSE, warning=FALSE>>=
library(raster)
library(rgdal)
library(sp)
@

\noindent The source files for the raster data can be stored in the working directory or loaded by specifying the exact path. The \texttt{raster()} function is a universal and very powerful tool for loading all kinds of raster data. For reading shapefiles, use the \texttt{readOGR()} function. Below is an example of how to read a set of raster layers.
<<>>=
# First, make sure that your working directory is still 
# the one specified earlier:
# getwd()

# Now read the layers (in our case the layers are stored in a different place):
ruggedness <- raster("P:/SSF PROJECT/NEW GIS LAYERS/tri1") 
landcover <- raster("P:/SSF PROJECT/NEW GIS LAYERS/lc_30") 
canopycover <- raster("P:/SSF PROJECT/NEW GIS LAYERS/cc_abmt") 
disthighway <- raster("P:/SSF PROJECT/NEW GIS LAYERS/disthwy")
distroad <- raster("P:/SSF PROJECT/NEW GIS LAYERS/distsmrd")

# It is enough to load the whole folder were your *.adf files are 
# stored in. The function raster() finds the needed files itself. 
@

\noindent You can plot the data for a first overview. As this can take a while with large datasets, we outcommented the following chunk.
<<eval=FALSE>>=
plot(ruggedness) 
plot(landcover)
plot(canopycover)
plot(disthighway)
plot(distroad)
@



\subsection{Raster Extraction}
\noindent Now that you have generated the random steps and loaded the raster data, you can take the next step and actually connect the observed and potential points with the spatial covariates. There are different functions that can do this. When choosing one, you need to consider that raster files are large and juggling with them occupies lots of memory and computing power. For this reason we suggest using the \texttt{extract()} function that allows for querying single pixel values without loading the whole source file into working memory. The code for compiling the final dataset involves three steps: 
\begin{itemize}
\item{Converting the \texttt{xmpl.steps} data frame into a Spatial Points Data frame}
\item{extracting the raster values}
\item{combining them to the final dataset}
\end{itemize}

\noindent Converting \texttt{xmpl.steps} into a ``SpatialPointsDataFrame'':
<<>>= 
xmpl.steps.spdf <- SpatialPointsDataFrame(coords = 
                                          xmpl.steps[,c("new_x","new_y")], 
                                          data = xmpl.steps)
@

\noindent Extracting the values from each raster layer:
<<>>= 
ruggedness.extr <- extract(ruggedness, xmpl.steps.spdf, 
                           method='simple',sp=F, df=T) 
canopycover.extr <- extract(canopycover, xmpl.steps.spdf, 
                            method='simple', sp=F, df=T)
disthighway.extr <- extract(disthighway, xmpl.steps.spdf, 
                            method='simple', sp=F, df=T)
distroad.extr <- extract(distroad, xmpl.steps.spdf, 
                         method='simple', sp=F, df=T)
landcover.extr <- extract(landcover, xmpl.steps.spdf, 
                          method='simple', sp=F, df=T)
@
\noindent The extraction is done separately for each layer. The option \texttt{method = 'simple'} extracts value from nearest cell whereas \texttt{method = 'bilinear'} interpolates from the four nearest cells. You can adjust this option according to the resolution of your dataset and ecological considerations. \texttt{df=T} returns the result as a data frame and \texttt{sp=F} ensures that the output is not added to the original dataset right away. 

\noindent Automatically adding the extracted values to the original dataset sounds like a handy option. For two reasons we do not use it here: Firstly, we want to set the column names manually for not ending up with several columns called ``w001001''. Secondly, our data include a categorial covariate (landcover) that we want to reclassify and flag as a factor.

\noindent This is the code for compiling the final dataset:
<<>>= 
xmpl.steps.spdf$ruggedness <- ruggedness.extr[,2]
xmpl.steps.spdf$canopycover <- canopycover.extr[,2]
xmpl.steps.spdf$disthighway <- disthighway.extr[,2]
xmpl.steps.spdf$distroad <- distroad.extr[,2]

# The landcover covariate comes coded in integers between 0 an 10 
# and is by default (mis)interpreted as an integer string. 

unique(landcover.extr[,2])
# Re-classifying landcover:
xmpl.steps.spdf$landcover <- as.factor(
    ifelse(landcover.extr[,2] == 0,NA,
    ifelse(landcover.extr[,2] < 5, "forest", 
    ifelse(landcover.extr[,2] < 8, "open",NA))))   
@
\noindent Now your final dataset should be ready for analysis. Examine it:  
<<>>=
head(xmpl.steps.spdf)
@

\subsection{Checking for Multicollinearity} % -------------------------------------------------------------------------------------------------------------------------------------------

\noindent Before including all environmental factors in your analysis, you should check if two or even more variables are exact or highly correlated. The threshold for the correlation coefficiant is 0.7 or higher. To create a correlation matrix, we first need to convert the ``Spatial Points Data Frame'' into a data frame. Furthermore the column \texttt{landcover} needs to be changed from ``facotor'' into ``numeric''. 


<<>>=

xmpl.steps.df <- as.data.frame(xmpl.steps.spdf)

Z <- subset(xmpl.steps.df, select=c(ruggedness,canopycover,disthighway,distroad))

Z$landcover <- as.numeric(
    ifelse(xmpl.steps.df$landcover == "forest",0,
    ifelse(xmpl.steps.df$landcover == "open", 1,NA)))

#head(Z)

cor(Z, use="pairwise.complete.obs")


#creating a nice plot

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y,use="pairwise.complete.obs"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}


pairs(Z, lower.panel=panel.smooth,
      upper.panel=panel.cor,diag.panel=panel.hist)
@

\noindent As you can see, the canopycover and the landcover are highly correlated. The correlation coefficient for the distance to roads and the distance to highways is very high as well. For our further analysis we therefore only use landcover and the distance to roads.


\section{Final SSF Model}

There are several options to analyze the generated data. As \cite{thurfjell2014applications} describe in their review of SSFs, conditional logistic regression has been the most commonly used procedure. Recently, researchers have tried to account for among-individual heterogeneity in their dataset. Thurfjell et al. \cite{thurfjell2014applications} recommend two R packages providing the necessary functionality: The \texttt{lme4} package and the \texttt{mclogit} package. For giving two simple examples, we will demonstrate data analysis with a mixed conditional logistic regression \texttt{(mclogit)} and a generalized linear mixed model \texttt{(lme4)}. 
We will test for effects of ruggedness, canopy cover, land cover, distance to road and distance to highway. The geographic covariates will be included as quadratic terms, resulting in the equation:
\[
\displaystyle case ~ 
ruggedness + ruggedness^2 + disthighway + disthighway^2 + distroad + distroad^2 + landcover
\]

Both models should produce rather similar outputs. Certainly, you are free to implement any type of model you find more elaborate or more suitable.

In a second step, we will use one of the fitted models to generate predictions and plot them.


\subsection{generalized linear mixed model}%------------------------------

family binomial (with a binomial distribution of error)
nested random effect for ID and strata
...where the random effect takes the form of (1|id/strata).

structure of pseudo replication: id, stratum



<<echo=TRUE, eval=FALSE>>=

xmpl.glmm.fit <- glmer(case ~ 
    landcover +
    ruggedness + I(ruggedness^2) + 
    distroad + I(distroad^2) + 
    (1|id/strata),
    family = binomial, data=xmpl.steps.df)

summary(xmpl.glmm.fit)

@
The model does not run and we get error messages suggesting us to scale the variables. 
Using the function \texttt{scale()} we center and scale the data so we get comparable values for each predictor. The according equation is: 
\[
\displaystyle f(x) = (x - mean)/SD(x)
\]


<<echo=TRUE, eval=TRUE>>=

library(lme4)
library(effects)

xmpl.glmm.fit.sc <- glmer(case ~ 
    landcover +
    scale(ruggedness) + I(scale(ruggedness)^2) + 
    scale(distroad) + I(scale(distroad)^2) + 
    (1|id/strata),
    family = binomial, data=xmpl.steps.df)


summary(xmpl.glmm.fit.sc)

plot(allEffects(xmpl.glmm.fit.sc))

@



\subsection{mixed conditional logistic regression}%---------------------------

The conventional logistic regression models do not take into account that in wildlife telemetry data observations are not independent but rather linked. If for example the selection differs among individual animals, the model may be flawed by pseudo replication. The mixed conditional logistic regression allows for specifying a random effects structure and thereby is capable of handling matched observations from different animals. 
Here is how to implement it in R:

<<>>=
library(mclogit)
# Trasform dataset to a data frame
xmpl.steps.df <- as.data.frame(xmpl.steps.spdf)

# Recall the column names of the dataset:
#head(xmpl.steps.df)

#xmpl.steps.spdf$id <- as.integer(xmpl.steps.spdf$id)

# column "case" indicates whether a site was visited or not. "strata" indicates the burst number and "id" the individual animal.

# The actual logistic model:
xmpl.logit.fit <- 
    mclogit(cbind(case, strata) ~ 
    landcover +
    ruggedness + I(ruggedness^2) + 
    distroad + I(distroad^2), random=~1|id,
    data = xmpl.steps.df, start.theta=1000)
 
## when including random effects we run into problems. We get the error message that the data do not have enough residual variance.

#random=~1|id,
#disthighway + I(disthighway^2),

summary(xmpl.logit.fit)

@


\subsection{Predictions}

Formula

\[
  \displaystyle w(x) = exp(\beta 1 * x1 + \beta 2 * x2 + ... + \beta p * xp)
  \]


\subsubsection{Predictions for GLMM}


-----------------PREDICTIONS FOR GLMM-------------------
<<>>=
# predictions for glmm

#plot(predict(xmpl.glmm.fit.sc, type="response"))
summary(xmpl.glmm.fit.sc)

mean(xmpl.steps.df$ruggedness)
sd(xmpl.steps.df$ruggedness)

summary(scale(xmpl.steps.df$ruggedness))

rm(mydata)
mydata = data.frame(ruggedness=seq(-1.5,15,0.1))
#mydata = data.frame(ruggedness=scale(xmpl.steps.df$ruggedness))

mydata$wruggforest = exp(              
              xmpl.glmm.fit.sc@beta[3] * mydata$ruggedness  + 
              xmpl.glmm.fit.sc@beta[4] * mydata$ruggedness^2 + 
              xmpl.glmm.fit.sc@beta[5] * median(scale(xmpl.steps.df$distroad)) + 
              xmpl.glmm.fit.sc@beta[6] * median((scale(xmpl.steps.df$distroad))^2))

mydata$wruggopen = exp(
              xmpl.glmm.fit.sc@beta[2] +
              xmpl.glmm.fit.sc@beta[3] * mydata$ruggedness  + 
              xmpl.glmm.fit.sc@beta[4] * mydata$ruggedness^2 + 
              xmpl.glmm.fit.sc@beta[5] * median(scale(xmpl.steps.df$distroad)) + 
              xmpl.glmm.fit.sc@beta[6] * median((scale(xmpl.steps.df$distroad))^2))


#plot(mydata$ruggedness, mydata$wruggforest, type="l", col="darkgreen", lwd=2)
#lines(mydata$ruggedness, mydata$wruggopen, type="l", col="green", lwd=2)

## rescaling


mydata$ruggednessResc <- (mydata$ruggedness * sd(xmpl.steps.df$ruggedness) + mean(xmpl.steps.df$ruggedness))


#head(mydata)
#str(mydata)
#mydata$ruggednessResc

plot(mydata$ruggednessResc, mydata$wruggforest, main="GLMM predictions for ruggedness", type="l", col="darkgreen", lwd=2, ylim=c(0,1), xlim=c(0,250), xlab="ruggedness", ylab="W", las=1)
lines(mydata$ruggednessResc, mydata$wruggopen, type="l", col="green", lwd=2)


#abline(h=1,lty=2,col="wheat4")

@


\subsubsection{Predictions for MCLOGIT}

-----------------PREDICTIONS FOR MCLOGIT-------------------
<<>>=
#predictions for mclogit

summary(xmpl.steps.df$ruggedness)
summary(xmpl.logit.fit)
#plot(predict(xmpl.logit.fit))

mydata = data.frame(ruggedness=seq(0,250,1))

mydata$wruggforest = exp(xmpl.logit.fit$coefficients[2] * mydata$ruggedness  + 
                xmpl.logit.fit$coefficients[3] * mydata$ruggedness^2 + 
                xmpl.logit.fit$coefficients[4] * median(xmpl.steps.df$distroad) +
                xmpl.logit.fit$coefficients[5] * (median(xmpl.steps.df$distroad))^2)

mydata$wruggopen = exp(xmpl.logit.fit$coefficients[1] +
                xmpl.logit.fit$coefficients[2] * mydata$ruggedness  + 
                xmpl.logit.fit$coefficients[3] * mydata$ruggedness^2 + 
                xmpl.logit.fit$coefficients[4] * median(xmpl.steps.df$distroad) +
                xmpl.logit.fit$coefficients[5] * (median(xmpl.steps.df$distroad))^2)

plot(mydata$ruggedness,mydata$wruggforest, main="MCLOGIT predictions for ruggedness", type="l", col="darkgreen", lwd=2, ylim=c(0,1), xlab="ruggedness", ylab="W", las=1)
lines(mydata$ruggedness,mydata$wruggopen,type="l", col="green", lwd=2)
#abline(h=1,lty=2,col="wheat4")

@





# save models -----------------------
<<>>=
save(pippo, file="pippo.Rdata")
pippo <- load("pippo.Rdata")

@




\newpage
\section{Acknowledgements}
Don't forget to thank TeX and R and other opensource communities if you use their products! The correct way to cite R is shown when typing ``\texttt{citation()}'', and ``\texttt{citation("mgcv")}'' for packages.

Special thanks to \ding{164} \ding{170} Simone \ding{170}\ding{170}\ding{170} \ding{164}\ding{165}\ding{95}! You were our best team member! \ding{96}

Save Models!
\newpage
\section{Appendix}

Session Info:
<<echo=FALSE>>=
sessionInfo()
@

\bibliography{References}{}
\bibliographystyle{plain}


=======
\documentclass[11pt, a4paper]{article} %or article has only section and below, book and report also have chapter: http://texblog.org/2007/07/09/documentclassbook-report-article-or-letter/

\usepackage[utf8]{inputenc}  % use utf8 encoding of symbols such as umlaute for maximal compatibility across platforms

\usepackage{caption}  			% provides commands for handling caption sizes etc.
%\usepackage[a4paper, left=25mm, right=25mm, top=25mm, bottom=25mm]{geometry}		 % to easily change margin widths: https://www.sharelatex.com/learn/Page_size_and_margins

\usepackage{etoolbox}    % for conditional evaluations!
\usepackage[bottom]{footmisc}  % I love footnotes! And they should be down at the bottom of the page!
\usepackage{graphicx}        % when using figures and alike
\usepackage[hidelinks]{hyperref}		% for hyperreferences (links within the document: references, figures, tables, citations)

\usepackage{euler}     % a math font, only for equations and alike; call BEFORE changing the main font; alternatives: mathptmx, fourier, 
%\usepackage{gentium} % for a different font; you can also try: cantarell, charter, libertine, gentium, bera, ... http://tex.stackexchange.com/questions/59403/what-font-packages-are-installed-in-tex-live

\usepackage{pifont}

%\usepackage{listings}
%\lstset{breaklines=T}

%------------------------------------------------------------------------------------------------------
%------- text size settings --------------
\setlength{\textwidth}{16cm}% 
\setlength{\textheight}{25cm} %23 
%(these values were used to fill the page more fully and thus reduce the number of pages!)
\setlength{\topmargin}{-1.5cm} %-1.5
\setlength{\footskip}{1cm} %
%\setlength{\hoffset}{0cm} %
\setlength{\oddsidemargin}{0cm}%1
\setlength{\evensidemargin}{0cm} %-0.5
\setlength{\parskip}{0cm} % Abstand zwischen Absätzen
% ----------------------------------------------------------------
\renewcommand{\textfraction}{0.1} % allows more space to graphics in float
\renewcommand{\topfraction}{0.85}
%\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.70}


\frenchspacing %http://texwelt.de/wissen/fragen/1154/was-ist-french-spacing-was-macht-frenchspacing

\sloppy

%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------

\begin{document}

%%%%%%%%%%%%% this bit is new to Knitr: %%%%%%%%%%%%%%%%%%%%%
<<setup, include=FALSE, cache=FALSE, echo=F>>=
library(knitr)
# library(formatR)
# set global chunk options
opts_chunk$set(fig.path='', fig.align='center', fig.show='hold', tidy=FALSE)
options(replace.assign=TRUE, width=80)
#render_listings()
@


\title{A tutorial for implementing Step Selection Function in R}

\author{P. Antkowiak\thanks{M.Sc. programme ``GIS und Umweltmodellierung'' at University of Freiburg} \and H. Tripke\thanks{M.Sc. programme ``Wildlife, Biodiversity and Vegetation'' at University of Freiburg} \and C. Wilhelm\thanks{M.Sc. programme ``Wildlife, Biodiversity and Vegetation'' at University of Freiburg}}
% for more control, multiple affiliations, line breaks and alike, use the authblk package!!

\date{\today} % !!use package isodate for more control of date formatting!!

\maketitle

%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------

\tableofcontents

\newpage

\section{Introduction}%------------------------------------------------------------------------------------------------------

\subsection{Purpose and applications of SSFs}

\noindent In addition to Resources Selection Functions (RSF) another powerful tool for evaluating data on animal movements and habitat selection are Step Selection Functions (SSF). The latter are used to estimate resource selection by comparing observed habitat use with available structures. Given GPS locations of a collared individual are connected by a linear segment. These segments are considered as steps. The fix rate of the GPS locations that influence the step length should be choosen carefully (i.e. by conducting a pilot study) to meet the requirements of the study questions, the target species and its behaviour. Then random steps are calculated by taking measured angle and distance along steps and using the observed positions as starting points. These alternative steps represent the available habitat, which could have been chosen, within a realistic step length of the observed positions. Finally, we can compare spatial attributes on  both and test for effects that explain habitat selection by animals \cite{thurfjell2014applications}. \\ So far, SSF models were mainly done using Geospatial Modelling Environment (GME) that works with a GIS\footnote{www.spatialecology.com/gme/}. Moreover, many packages for analyzing animal movements are provided in R. None of these packages is designed for doing a SSF only or is at least missing fundamental steps. For example, the \texttt{adehabitatLT} has many features for analyzing your telemetry data (note that reading the help from 2011 is very much recommendend by us \cite{Package2011}!) but is missing any function to convert random steps. Another problem we faced when exploring the available packages in R was that some of them are lacking detailed descriptions on how to use their examples. The data for these examples is well prepared but for an user, who is freshly starting with SSF, it is hard to understand the structure of these data.\\ Therefore, the aim of this tutorial is to collect all these functions necessary to conduct a SSF and order and describe them in a  way that intuitively makes you understand how to run a SSF with your own data. Each step will be explained using an exemplary dataset of GPS locations collected from seven Cougars (\textit{Puma concolor}) in the year 2010 (in the following adressed as ``xmpl''). Since the study design is fundemental to successfully implement SSFs, we recommend to read  Thurfjell et al. (2014) \cite{thurfjell2014applications}, who provide different options and suggestions for conducting SSFs, before starting your own study.\\

\begin{figure} 
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{explain_a_burst.png} 
  \caption{Animal movements in an environment. Theses data are already separated into bursts of not more than 3h 10min intervals. This prohibits to combine positions uncared of missing data points. These bursts provide the angle and distance to calculate random steps from. Thereby, strata and case have to be defined.}
\label{fig:Burst}
\end{figure}


\subsection{Our SSF workflow in R}

\noindent Figure~\ref{fig:Flowchart} provides an overview of all necessary steps and potential options to conduct a SSF. This tutorial will guide you through each step and gives brief instructions on how to implement the functions and what to consider beforehand.
To conduct a SSF using this tutorial we need you to store your initial data  in two independant datasets: \begin{enumerate} \item {A raster file of your spatial attributes (\emph{Raster data})} and \item{GPS locations of your individuals assigned with a time stamp (\emph{Waypoint data})}. \end{enumerate} 
We will start with the \emph{Waypoint data} because these need to be transformed a couple of times to be able to work with them. You can find the single steps on the right site of Figure~\ref{fig:Flowchart}. While there are many options to adjust your \emph{Waypoint data} the \emph{Raster data} describing your spatial attributes needs not much of change. Once you created random steps for your observed positions you can extract the spatial attributes for each of those positions by using the function \texttt{extract}. At this point, \emph{Waypoint} and \emph{Raster data} will be combined and your final model can be written.   


\begin{figure} 
\captionsetup{width=1\textwidth}
\centering
\includegraphics[width=1\textwidth]{Flowchart.pdf} % our perfect workflow!
  \caption{Stepwise conducting a Step Selection Function using existing R-packages. The yellow boxes show the name of the function applied while the blue boxes provide the type of object or data. In light grey optional steps are highlighted that are not implemented in this tutorial but should be considered by doing SSF.}
\label{fig:Flowchart}
\end{figure}


\subsection{Installing and loading Packages}

\noindent Before you can actually start using this tutorial for conducting SSF you need to install a bunch of packages in R. For faster processing the \texttt{install.packages} function is deactivated.\\
  
  << eval=FALSE, warning=FALSE, message=FALSE>>=
  
  ## for implementing SSF
  install.packages("hab")
install.packages("hab", repos = "http://ase-research.org/R/") # regular
install.packages("hab", repos = "http://ase-research.org/R/",
                 type = "source")                             # for self-compiling
@
  

  
  << eval=FALSE, warning=FALSE, message=FALSE>>=
  install.packages("adehabitatHR") # dealing with home ranges
install.packages("adehabitatHS") # habitat selection
install.packages("adehabitatLT") # trajectories
install.packages("adehabitatMA") # maps

install.packages("tkrplot")

# for handling raster data
install.packages("move")
install.packages("raster")
install.packages("rgdal")

# for analyzing the data
install.packages("mclogit")
install.packages("lme4")
install.packages("effects")
@
\nointend Keep fingers off the \texttt{adehabitat} package! It is outdated and replaced by four different packages designed for different analyses.

\noindent Loading packages:
  
  << eval= TRUE, warning=FALSE, message=FALSE>>=
  library(hab)
library(adehabitatMA)
library(adehabitatHR)
library(adehabitatHS)
library(adehabitatLT)

library(sp)
library(raster)

library(mclogit)
library(lme4)
library(effects)
@



\section{Processing the Waypoint Data}

\subsection{Loading Waypoint Data (*.csv, ESRI)}%------------------------------------------------------------------------------------------------------

\noindent The data for the analysis should be saved in a simple *.csv file format. The table should have headings for each column in the first row and each observation should include at least the four following values: 
\begin{itemize}
\item{animal ID}
\item{x-coordinate (easting)}
\item{y-coordinate (northing)}
\item{date and/or time}
\end{itemize}

\noindent Remember that the coordinates need to be provided in the same coordinate system and spatial projection as the raster data.\\

\noindent Depending on your analysis you can include further values such as:
\begin{itemize}
\item{ID for each record}
\item{GPS precision}
\item{other recording parameters such as season or month}
\item{temperature / elevation at the moment of record}
\item{other values that might be of interest in the further analysis}
\end{itemize}


\noindent Use the following commands to set your working directory and read the data:
<<>>=
setwd("P:/Henriette/BestPracticeR/SSF-workflow/Code")
xmpl = read.csv("UTMsREDUCED.csv", head=T)
@

\noindent You can execute \texttt{head(xmpl)} and \texttt{str(xmpl)} to check, whether the data was successfully read.


\subsection{Creating a ``Spatial Points Data Frame''}%------------------------------------------------------------------------------------------------------
\noindent The functions used along the rest of the toolchain can only process data which is stored as an object of class ``SpatialPointsDataFrame''. This object class stores the coordinates separately and can be created using the according function from package \texttt{sp}. 

<< warning=FALSE, message=FALSE>>=
library(sp)

xmpl.spdf = SpatialPointsDataFrame(coords = xmpl[,c("easting","northing")],
                                   data = xmpl)
@
\noindent In the \texttt{coords} argument of the function you should specify the ``x'' and ``y'' values for your dataset. In our example, we simply assign the two coordinate columns of the example dataset, but you can read the coordinates from a separate file if you want.\\

\noindent To see, what information is stored in the data frame, use the \texttt{names} command:

<< warning=FALSE, message=FALSE>>=
names(xmpl)
@


\subsection{Creating an ``ltraj'' object}%------------------------------------------------------------------------------------------------------
\noindent After storing the data in a ``Spatial Points Data Frame'', you now need to connect the single points and turn them into a set of trajectories. This operation is carried out by the function \texttt{as.ltraj} from the``hab'' package and produces objects of class ``ltraj''. There are two different types of trajectories. For \textbf{Type I} the time is not known or not taken into account. \textbf{Type II} is characterized by a timestamp. If the time lag between the different locations is the same, it is called ``regular'', if not ``irregular''.
The function \texttt{as.ltraj} requires at least three arguments to work:
\begin{enumerate}
\item{``x'' and ``y'' (x- and y- coordinates for each point)}
\item{``date'' (timestamp for each point, given as ``POSIXct'' class)}
\item{``id'' (the animal id)}
\end{enumerate}

\noindent Both, coordinates and animal id can easily be adopted from the ``Spatial Points Data Frame''. The timestamp however, needs to be stored as a ``POSIXct'' value with date and time in the same cell. If it is not stored in the required format yet, you therefore need to convert it first:

<<>>=
date <- as.POSIXct(strptime(paste(xmpl.spdf$LMT_DATE, xmpl.spdf$LMT_TIME),
                            "%d/%m/%Y  %H:%M:%S"))
@
\noindent If your dataset already features a ``POSIXct'' timestamp, you can skip this step.
Now you can proceed and actually create the ``ltraj'' object by executing the following command:
<<>>=
xmpl.ltr <- hab:::as.ltraj(xy = xmpl.spdf@coords, date = date,
                           id = xmpl.spdf$cat) 
@
\noindent Two comments to the function used: By typing \texttt{hab:::as.ltraj} you tell R to use the \texttt{as.ltraj} function from the ``hab'' package which is speed optimized against its \texttt{adehabitatLT} sibling. Unlike the \texttt{xmpl.spdf@coords} promt which works for any ``SPDF'' object, the \texttt{xmpl.spdf\$cat} prompt is specific to your dataset. In the example dataset, animal ID's are stored as an integer vector called \texttt{cat}. If this differs in your dataset and you should change the prompt accordingly. The \texttt{id} creates subsets in your data set which you can see in the structure below.

\noindent You now may want to have a closer look at the created ``ltraj'' object. Display its structure by executing \texttt{str(xmpl.ltr)}. The ``ltraj'' object is a list containing seven different dataframes, each for one individual, which we defined with the \texttt{id}. In each dataframe all information is stored for every single observation of the individual: 
\begin{itemize}
\item{\texttt{x} and ``y'' (x- and y- coordinates for each point)}
\item{\texttt{date} (timestamp for each point, given as POSIXct class)}
\item{\texttt{dx} and ``dy'' (changes in x- and y- values)}
\item{\texttt{dist} (length of trajectory)}
\item{\texttt{dt} (time between relocations in s)}
\item{\texttt{R2n} (squared net displacement between current relocation and first relocation)}
\item{\texttt{abs.angle} (absolute angle of trajectory)}
\item{\texttt{rel.angle} (angle between previous and actual trajectory)}
\end{itemize}

<<>>=
str(xmpl.ltr, width=80)
@

\noindent To get a visual impression of your data you can plot the trajectory for all or for one particular animal:

<<ltraj_plot, echo=TRUE, dev="pdf", fig.show="hide">>=
plot(xmpl.ltr)
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:ltraj_plot}.

\begin{figure}[!htbp]
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{ltraj_plot.pdf}
\caption{Visualization of the observed positions and the path from seven collared cougars.}
\label{fig:ltraj_plot}
\end{figure}

\noindent To get a list of all cat IDs, use the \texttt{unique} command. Choose one that you are interested in!
<<>>=
unique(xmpl.spdf$cat) 
    
@

\noindent You can then look at the trajectory of a single individual only.
<<ind_plot, echo=TRUE, dev="pdf", fig.show="hide">>=
plot(xmpl.ltr, id=10289)
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:ind_plot}.

\begin{figure}[!htbp]
\captionsetup{width=0.6\textwidth}
\centering
\includegraphics[width=0.6\textwidth]{ind_plot.pdf}
\caption{Visualization of the observed positions and the path from only one individual.}
\label{fig:ind_plot}
\end{figure}


\subsection{Creating Bursts}%

\noindent As described in the previous section, the relocations stored in the ``ltraj'' object are already devided into the different individuals. This partition is called a ``burst''. For analysing the data, there might be the need to create ``sub-bursts'' for each animal within your trajectory. For example, if the animals were only recorded during the day, the monitoring took place over two consecutive years or the time lag between the relocations differs remarkably, each accumulation of relocations can be defined as a different burst. Looking at those different parts seperately might be necessary for different reasons. It is also possible to split your data into homogenous behaviour. Suggestions how to do that are explained in one of the following sections.\\
\noindent The function \texttt{cutltraj} splits the given bursts of your ``ltraj'' object into smaller bursts according to a specified criterion. In contrast, the function \texttt{bindltraj} combines the bursts of an object of class ``ltraj'' with the same attribute \texttt{id} to one unique burst. 


\noindent To find out if there are missing values and to get an overview of the time lags between the relocations, you can plot the changes of your trajectory over time \texttt{dt}. For that, you need to define the time interval you are looking at \cite{0.3.162014}.

\noindent In our example, the locations of the cougars were recorded every 3 hours, starting at 3 AM. The location at midnight is always missing. As \texttt{dt}, the time between successive relocations, is measured in seconds, we need to convert it into 3 hours.

<<time_seq, echo=TRUE, dev="pdf", fig.show="hide">>=
plotltr(xmpl.ltr, "dt/3600/3")
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:time_seq}. The relocations which are 3 hours apart, are plottet at value 1 on the y-axis, if one relocation is missing (time lag of 6 hours) the relocation gets the value 2. As you can see, there are a lot of relocations missing. To keep the time lag constant, we now want to split the existing bursts (individuals) into ``sub-bursts'' where the time lag is bigger than 3 hours. A constant time lag is necessary to have comparable trajectories for further analyses.

\begin{figure}[!htbp]
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{time_seq.pdf}
\caption{For each individual the observations are plotted. The y-axis shows the time interval from one observation to the next. Observations at 1 means that they are not more than 3 hours apart.}
\label{fig:time_seq}
\end{figure}

\noindent To cut our data at our desired interval, we need a function which defines \texttt{dt}. Because we want to keep relocations which are only a few minutes ``wrong'', we added 10 extra minutes.\\
\noindent Note: We still keep data with different time lags (< 3:10 h). As this is an irregular type II trajectory, it might cause problems with different functions (e.g. checking for autocorrelation). There are functions to convert it into a regular one, look at the \texttt{adehabitatLT} tutorial by C. Calenge \cite{0.3.162014}, the help section to get more information or define an explicit time lag in the function below. Because this however might lead to a massive loss of data, we didn't do that. 

<<>>=
foo = function(dt) { return(dt> (3800*3))} 
@

\noindent Then we split the object of class ltraj into smaller bursts using \texttt{cutltraj} and the function above. The bursts we had before applying this function still remain.

<<>>=
xmpl.cut <- cutltraj(xmpl.ltr, "foo(dt)", nextr = TRUE)
@
\noindent Note the Warning Message! As many recordings are missing we are losing a lot of data.

\noindent There are two options of cutting the trajectory depending on nextr. If it is set as FALSE, the burst stops before the first relocation matching the criterion. if it is set as TRUE, it stops after. \cite{0.3.162014}


\subsection{Distinguish different behaviors: Broken stick model and autocorrelation}
NEEDS EXPLANATION!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Autocorrelation between dist or angle means homogenous behaviour

\subsection{Creating Random Steps}

\noindent Given your final bursts we now randomly draw angles and distances from your observed data to get random steps. The angle is taken from the previous trajectory to the actual one while the distance is taken from the starting point to the next observed position. This means that each burst should at least contain three observed positions to create random steps for one relocation. The first and the last relocations only contribute to the calculation but don't have their ``own'' random steps as there is no previous trajectory an angle could be calculated from. Neither there is an endpoint to draw a distance. This means that the number of poins with random steps is lower as your actual recorded relocations.
Before applying the function \texttt{rdSteps} you might want to check for correletaion between turning angle and distance. In case your individuals tend to move long distances by turning only in small angles (e.g. a species migrating) but stop for several days for feeding you want to pick the distance and the angle as pairs. If no correlation is found you can pick both variables independently.
To check for this we use the \texttt{plot} function but first have to convert our ``ltraj'' object back to a data frame by using \texttt{ld}.

<<>>=
# ld is a quick way to create a data frame from an ltraj object
xmpl.cut.df <- ld(xmpl.cut)  
   
@


<<cor_1, echo=TRUE, dev="pdf", fig.show="hide">>=
plot(xmpl.cut.df$dist, xmpl.cut.df$rel.angle)
@
\noindent The result of this code chunk is shown in Figure~\ref{fig:cor_1}.

\begin{figure}[!htbp]
\captionsetup{width=0.8\textwidth}
\centering
\includegraphics[width=0.6\textwidth]{cor_1.pdf}
\caption{Testing for correlation of the observed turning angle and step length. The y-axis shows the difference in the turning angle. The shape indicates a correlation for longer steps and doing only small turning angles (around 1).}
\label{fig:cor_1}
\end{figure}


\noindent The plot shows a correlation of step length and turning angle and therefore the random steps should be taken as pairs (\texttt{simult = T}). Per default the angle and distance for each random step is drawn from the observed values you provide with \texttt{x}. If your random steps shall be taken from a different dataset you can do so by writting it in \texttt{rand.dist = YourDataSet}. Hereby, you can also specify a distribution for estimating angle and distance. In our case we stick to the same dataset and apply \texttt{rdSteps}.  

<<>>=
  
xmpl.steps <- rdSteps(x = xmpl.cut, nrs = 10, simult = T, rand.dis = NULL,  
                      distMax = Inf, reproducible = TRUE, only.others = FALSE) 
          # use simult = FALSE if your data is not correlated
@

\noindent The function \texttt{rdSteps} uses some default settings which offer you options to modify your random steps. You can for example, easily change the number of steps taken from the observed data by defining \texttt{nrs} (default is 10) or if you only need steps shorter than a certain value specify \texttt{distMax} to that value (per default all steps are taken). By setting \textt{reproducible = TRUE} a seed is used to get reproducible random steps. If you want to exclude your current individual to draw angle and distance from this set include \texttt{only.others = TRUE}.
\noindent All in all, \texttt{rdSteps} is very straight forward and computes a lot of useful things for you: 

<<>>=
head(xmpl.steps)
@

\noindent The table still includes your ``cat id'', ``burst id'', the ``rel.angle'' and ``dist'' of your observed positions. Furthermore, the ``case'' is provided as categories of 0 and 1 for available and used. The ``strata'' marks all 10 random steps and the visited location so you can later tell your function what to compare.  
Depending on your analysis you might want to compare only your observed positions with the endpoints of your random steps or you want to investigate in the selection of spatial attributes along the path. For latter you need to implement more code because we did not use this option in our tutorial (see the grey box in Figure~\ref{fig:Flowchart}).\\
Now only new coordinates for your random points are missing. Instead two columns provide the differences of your x- and y- coordinates for each random step (``dx'' and ``dy''). To get new coordinates for your random steps we simply add these differences to your initial coordinates and create two new columns. 

<<>>=
xmpl.steps$new_x <- xmpl.steps$x + xmpl.steps$dx
xmpl.steps$new_y <- xmpl.steps$y + xmpl.steps$dy
@
  
\noindent After running this chapter you get your final ``SpatialPointDataFrame'' to use with the selection function.

<<>>=
head(xmpl.steps)
@


\subsection{Different distributions as an option for rdSteps}

!!!!!!!!!!!!!!!NEEDS FURTHER EXPLANATION!!!!!!!!!!!!!!!!!!!!!!!!!!
\\


\section{Processing Spatial Covariates}%----------------------------------------------------------------------------------------------------------------------------
\noindent This section explains the handling of spatial parameters that will be tested for selection by the target species. You should store these data in raster files (ESRI *.adf or georeferenced *.tif). These should have the same coordinate system as your telemetry data and should (for time reasons) already be clipped to your study area. For instructions how to do this in R, please read the GIS instructions from the other group ;)   

\subsection{Load Raster Data (ESRI, *.tif, (*.shp))}%------------------------------------------------------------------------------------------------------

\noindent With a simple function stored in the package \texttt{raster} you are able to upload any raster file into R. Examplarily we use raster data on the following parameters for the study area:
\begin{itemize}
\item{ruggedness of the terrain}
\item{land cover}
\item{canopy cover}
\item{distance to the nearest highway}
\item{distance to the nearest road}
\end{itemize}

\noindent For reading the raster data, three packages are required:
<<echo=FALSE, message=FALSE, warning=FALSE>>=
library(raster)
library(rgdal)
library(sp)
@

\noindent The source files for the raster data can be stored in the working directory or loaded by specifying the exact path. The \texttt{raster()} function is a universal and very powerful tool for loading all kinds of raster data. For reading shapefiles, use the \texttt{readOGR()} function. Below is an example of how to read a set of raster layers.
<<>>=
# First, make sure that your working directory is still 
# the one specified earlier:
# getwd()

# Now read the layers (in our case the layers are stored in a different place):
ruggedness <- raster("P:/SSF PROJECT/NEW GIS LAYERS/tri1") 
landcover <- raster("P:/SSF PROJECT/NEW GIS LAYERS/lc_30") 
canopycover <- raster("P:/SSF PROJECT/NEW GIS LAYERS/cc_abmt") 
disthighway <- raster("P:/SSF PROJECT/NEW GIS LAYERS/disthwy")
distroad <- raster("P:/SSF PROJECT/NEW GIS LAYERS/distsmrd")

# It is enough to load the whole folder were your *.adf files are 
# stored in. The function raster() finds the needed files itself. 
@

\noindent You can plot the data for a first overview. As this can take a while with large datasets, we outcommented the following chunk.
<<eval=FALSE>>=
plot(ruggedness) 
plot(landcover)
plot(canopycover)
plot(disthighway)
plot(distroad)
@



\subsection{Raster Extraction}
\noindent Now that you have generated the random steps and loaded the raster data, you can take the next step and actually connect the observed and potential points with the spatial covariates. There are different functions that can do this. When choosing one, you need to consider that raster files are large and juggling with them occupies lots of memory and computing power. For this reason we suggest using the \texttt{extract()} function that allows for querying single pixel values without loading the whole source file into working memory. The code for compiling the final dataset involves three steps: 
\begin{itemize}
\item{Converting the \texttt{xmpl.steps} data frame into a Spatial Points Data frame}
\item{extracting the raster values}
\item{combining them to the final dataset}
\end{itemize}

\noindent Converting \texttt{xmpl.steps} into a ``SpatialPointsDataFrame'':
<<>>= 
xmpl.steps.spdf <- SpatialPointsDataFrame(coords = 
                                          xmpl.steps[,c("new_x","new_y")], 
                                          data = xmpl.steps)
@

\noindent Extracting the values from each raster layer:
<<>>= 
ruggedness.extr <- extract(ruggedness, xmpl.steps.spdf, 
                           method='simple',sp=F, df=T) 
canopycover.extr <- extract(canopycover, xmpl.steps.spdf, 
                            method='simple', sp=F, df=T)
disthighway.extr <- extract(disthighway, xmpl.steps.spdf, 
                            method='simple', sp=F, df=T)
distroad.extr <- extract(distroad, xmpl.steps.spdf, 
                         method='simple', sp=F, df=T)
landcover.extr <- extract(landcover, xmpl.steps.spdf, 
                          method='simple', sp=F, df=T)
@
\noindent The extraction is done separately for each layer. The option \texttt{method = 'simple'} extracts value from nearest cell whereas \texttt{method = 'bilinear'} interpolates from the four nearest cells. You can adjust this option according to the resolution of your dataset and ecological considerations. \texttt{df=T} returns the result as a data frame and \texttt{sp=F} ensures that the output is not added to the original dataset right away. 

\noindent Automatically adding the extracted values to the original dataset sounds like a handy option. For two reasons we do not use it here: Firstly, we want to set the column names manually for not ending up with several columns called ``w001001''. Secondly, our data include a categorial covariate (landcover) that we want to reclassify and flag as a factor.

\noindent This is the code for compiling the final dataset:
<<>>= 
xmpl.steps.spdf$ruggedness <- ruggedness.extr[,2]
xmpl.steps.spdf$canopycover <- canopycover.extr[,2]
xmpl.steps.spdf$disthighway <- disthighway.extr[,2]
xmpl.steps.spdf$distroad <- distroad.extr[,2]

# The landcover covariate comes coded in integers between 0 an 10 
# and is by default (mis)interpreted as an integer string. 

unique(landcover.extr[,2])
# Re-classifying landcover:
xmpl.steps.spdf$landcover <- as.factor(
    ifelse(landcover.extr[,2] == 0,NA,
    ifelse(landcover.extr[,2] < 5, "forest", 
    ifelse(landcover.extr[,2] < 8, "open",NA))))   
@
\noindent Now your final dataset should be ready for analysis. Examine it:  
<<>>=
head(xmpl.steps.spdf)
@

\subsection{Checking for Multicollinearity} % -------------------------------------------------------------------------------------------------------------------------------------------

\noindent Before including all environmental factors in your analysis, you should check if two or even more variables are exact or highly correlated. The threshold for the correlation coefficiant is 0.7 or higher. To create a correlation matrix, we first need to convert the ``Spatial Points Data Frame'' into a data frame. Furthermore the column \texttt{landcover} needs to be changed from ``facotor'' into ``numeric''. 


<<>>=

xmpl.steps.df <- as.data.frame(xmpl.steps.spdf)

Z <- subset(xmpl.steps.df, select=c(ruggedness,canopycover,disthighway,distroad))

Z$landcover <- as.numeric(
    ifelse(xmpl.steps.df$landcover == "forest",0,
    ifelse(xmpl.steps.df$landcover == "open", 1,NA)))

#head(Z)

cor(Z, use="pairwise.complete.obs")


#creating a nice plot

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y,use="pairwise.complete.obs"))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}


pairs(Z, lower.panel=panel.smooth,
      upper.panel=panel.cor,diag.panel=panel.hist)
@

\noindent As you can see, the canopycover and the landcover are highly correlated. The correlation coefficient for the distance to roads and the distance to highways is very high as well. For our further analysis we therefore only use landcover and the distance to roads.


\section{Final SSF Model}

There are several options to analyze the generated data. As \cite{thurfjell2014applications} describe in their review of SSFs, conditional logistic regression has been the most commonly used procedure. Recently, researchers have tried to account for among-individual heterogeneity in their dataset. Thurfjell et al. \cite{thurfjell2014applications} recommend two R packages providing the necessary functionality: The \texttt{lme4} package and the \texttt{mclogit} package. For giving two simple examples, we will demonstrate data analysis with a mixed conditional logistic regression \texttt{(mclogit)} and a generalized linear mixed model \texttt{(lme4)}. 
We will test for effects of ruggedness, canopy cover, land cover, distance to road and distance to highway. The geographic covariates will be included as quadratic terms, resulting in the equation:
\[
\displaystyle case ~ 
ruggedness + ruggedness^2 + disthighway + disthighway^2 + distroad + distroad^2 + landcover
\]

Both models should produce rather similar outputs. Certainly, you are free to implement any type of model you find more elaborate or more suitable.

In a second step, we will use one of the fitted models to generate predictions and plot them.


\subsection{generalized linear mixed model}%------------------------------

family binomial (with a binomial distribution of error)
nested random effect for ID and strata
...where the random effect takes the form of (1|id/strata).

structure of pseudo replication: id, stratum



<<echo=TRUE, eval=FALSE>>=

xmpl.glmm.fit <- glmer(case ~ 
    landcover +
    ruggedness + I(ruggedness^2) + 
    distroad + I(distroad^2) + 
    (1|id/strata),
    family = binomial, data=xmpl.steps.df)

summary(xmpl.glmm.fit)

@
The model does not run and we get error messages suggesting us to scale the variables. 
Using the function \texttt{scale()} we center and scale the data so we get comparable values for each predictor. The according equation is: 
\[
\displaystyle f(x) = (x - mean)/SD(x)
\]


<<echo=TRUE, eval=TRUE>>=

library(lme4)
library(effects)

xmpl.glmm.fit.sc <- glmer(case ~ 
    landcover +
    scale(ruggedness) + I(scale(ruggedness)^2) + 
    scale(distroad) + I(scale(distroad)^2) + 
    (1|id/strata),
    family = binomial, data=xmpl.steps.df)


summary(xmpl.glmm.fit.sc)

plot(allEffects(xmpl.glmm.fit.sc))

@



\subsection{mixed conditional logistic regression}%---------------------------

The conventional logistic regression models do not take into account that in wildlife telemetry data observations are not independent but rather linked. If for example the selection differs among individual animals, the model may be flawed by pseudo replication. The mixed conditional logistic regression allows for specifying a random effects structure and thereby is capable of handling matched observations from different animals. 
Here is how to implement it in R:

<<>>=
library(mclogit)
# Trasform dataset to a data frame
xmpl.steps.df <- as.data.frame(xmpl.steps.spdf)

# Recall the column names of the dataset:
#head(xmpl.steps.df)

#xmpl.steps.spdf$id <- as.integer(xmpl.steps.spdf$id)

# column "case" indicates whether a site was visited or not. "strata" indicates the burst number and "id" the individual animal.

# The actual logistic model:
xmpl.logit.fit <- 
    mclogit(cbind(case, strata) ~ 
    landcover +
    ruggedness + I(ruggedness^2) + 
    distroad + I(distroad^2), random=~1|id,
    data = xmpl.steps.df, start.theta=1000)
 
## when including random effects we run into problems. We get the error message that the data do not have enough residual variance.

#random=~1|id,
#disthighway + I(disthighway^2),

summary(xmpl.logit.fit)

@


\subsection{Predictions}

Formula

\[
  \displaystyle w(x) = exp(\beta 1 * x1 + \beta 2 * x2 + ... + \beta p * xp)
  \]


\subsubsection{Predictions for GLMM}


-----------------PREDICTIONS FOR GLMM-------------------
<<>>=
# predictions for glmm

#plot(predict(xmpl.glmm.fit.sc, type="response"))
summary(xmpl.glmm.fit.sc)

mean(xmpl.steps.df$ruggedness)
sd(xmpl.steps.df$ruggedness)

summary(scale(xmpl.steps.df$ruggedness))

rm(mydata)
mydata = data.frame(ruggedness=seq(-1.5,15,0.1))
#mydata = data.frame(ruggedness=scale(xmpl.steps.df$ruggedness))

mydata$wruggforest = exp(              
              xmpl.glmm.fit.sc@beta[3] * mydata$ruggedness  + 
              xmpl.glmm.fit.sc@beta[4] * mydata$ruggedness^2 + 
              xmpl.glmm.fit.sc@beta[5] * median(scale(xmpl.steps.df$distroad)) + 
              xmpl.glmm.fit.sc@beta[6] * median((scale(xmpl.steps.df$distroad))^2))

mydata$wruggopen = exp(
              xmpl.glmm.fit.sc@beta[2] +
              xmpl.glmm.fit.sc@beta[3] * mydata$ruggedness  + 
              xmpl.glmm.fit.sc@beta[4] * mydata$ruggedness^2 + 
              xmpl.glmm.fit.sc@beta[5] * median(scale(xmpl.steps.df$distroad)) + 
              xmpl.glmm.fit.sc@beta[6] * median((scale(xmpl.steps.df$distroad))^2))


#plot(mydata$ruggedness, mydata$wruggforest, type="l", col="darkgreen", lwd=2)
#lines(mydata$ruggedness, mydata$wruggopen, type="l", col="green", lwd=2)

## rescaling


mydata$ruggednessResc <- (mydata$ruggedness * sd(xmpl.steps.df$ruggedness) + mean(xmpl.steps.df$ruggedness))


#head(mydata)
#str(mydata)
#mydata$ruggednessResc

plot(mydata$ruggednessResc, mydata$wruggforest, main="GLMM predictions for ruggedness", type="l", col="darkgreen", lwd=2, ylim=c(0,1), xlim=c(0,250), xlab="ruggedness", ylab="W", las=1)
lines(mydata$ruggednessResc, mydata$wruggopen, type="l", col="green", lwd=2)


#abline(h=1,lty=2,col="wheat4")

@


\subsubsection{Predictions for MCLOGIT}

-----------------PREDICTIONS FOR MCLOGIT-------------------
<<>>=
#predictions for mclogit

summary(xmpl.steps.df$ruggedness)
summary(xmpl.logit.fit)
#plot(predict(xmpl.logit.fit))

mydata = data.frame(ruggedness=seq(0,250,1))

mydata$wruggforest = exp(xmpl.logit.fit$coefficients[2] * mydata$ruggedness  + 
                xmpl.logit.fit$coefficients[3] * mydata$ruggedness^2 + 
                xmpl.logit.fit$coefficients[4] * median(xmpl.steps.df$distroad) +
                xmpl.logit.fit$coefficients[5] * (median(xmpl.steps.df$distroad))^2)

mydata$wruggopen = exp(xmpl.logit.fit$coefficients[1] +
                xmpl.logit.fit$coefficients[2] * mydata$ruggedness  + 
                xmpl.logit.fit$coefficients[3] * mydata$ruggedness^2 + 
                xmpl.logit.fit$coefficients[4] * median(xmpl.steps.df$distroad) +
                xmpl.logit.fit$coefficients[5] * (median(xmpl.steps.df$distroad))^2)

plot(mydata$ruggedness,mydata$wruggforest, main="MCLOGIT predictions for ruggedness", type="l", col="darkgreen", lwd=2, ylim=c(0,1), xlab="ruggedness", ylab="W", las=1)
lines(mydata$ruggedness,mydata$wruggopen,type="l", col="green", lwd=2)
#abline(h=1,lty=2,col="wheat4")

@





# save models -----------------------
<<>>=
save(pippo, file="pippo.Rdata")
pippo <- load("pippo.Rdata")

@




\newpage
\section{Acknowledgements}
Don't forget to thank TeX and R and other opensource communities if you use their products! The correct way to cite R is shown when typing ``\texttt{citation()}'', and ``\texttt{citation("mgcv")}'' for packages.

Special thanks to \ding{164} \ding{170} Simone \ding{170}\ding{170}\ding{170} \ding{164}\ding{165}\ding{95}! You were our best team member! \ding{96}

Save Models!
\newpage
\section{Appendix}

Session Info:
<<echo=FALSE>>=
sessionInfo()
@

\bibliography{References}{}
\bibliographystyle{plain}


>>>>>>> 76d03e8a17c038f92ccd8cde6c70f0cd1ed4841a
\end{document}